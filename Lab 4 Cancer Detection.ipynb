{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"DL_Lab4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"nteract":{"version":"0.28.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tpnN_XZwq_vc"},"source":["<a href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["# Lab 4: Cancer Detection\n","\n","## Objective\n","* To build a dense prediction model\n","* To begin reading current papers in DNN research\n","\n","## Deliverable\n","For this lab, you will turn in a notebook that describes your efforts at creating\n","a pytorch radiologist. Your final deliverable is a notebook that has (1) a deep network,\n","(2) method of calculating accuracy, \n","(3) images that show the dense prediction produced by your network on the pos_test_000072.png image (index 172 in the validation dataset).\n","This is an image in the test set that your network will not have seen before.\n","This image, and the ground truth labeling, is shown below.\n","(And is contained in the downloadable dataset below).\n","\n","![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=200&tok=a8ac31&media=cs501r_f2016:pos_test_000072_output.png)\n","<img src=\"http://liftothers.org/dokuwiki/lib/exe/fetch.php?media=cs501r_f2016:pos_test_000072.png\" width=\"200\">\n","\n","\n","## Grading standards\n","Your notebook will be graded on the following:\n","* 40% Proper design, creation and debugging of a dense prediction network\n","* 20% Proper implementation of train/test set accuracy measure\n","* 20% Tidy visualizations of loss of your dense predictor during training\n","* 20% Test image output\n","\n","\n","## Data set\n","The data is given as a set of 1024×1024 PNG images. Each input image (in \n","the ```inputs``` directory) is an RGB image of a section of tissue,\n","and there a file with the same name (in the ```outputs``` directory) \n","that has a dense labeling of whether or not a section of tissue is cancerous\n","(white pixels mean “cancerous”, while black pixels mean “not cancerous”).\n","\n","The data has been pre-split for you into test and training splits.\n","Filenames also reflect whether or not the image has any cancer at all \n","(files starting with ```pos_``` have some cancerous pixels, while files \n","starting with ```neg_``` have no cancer anywhere).\n","All of the data is hand-labeled, so the dataset is not very large.\n","That means that overfitting is a real possibility.\n","\n","## Description\n","For a video including some tips and tricks that can help with this lab: [https://youtu.be/Ms19kgK_D8w](https://youtu.be/Ms19kgK_D8w)\n","For this lab, you will implement a virtual radiologist.\n","You are given images of possibly cancerous tissue samples, \n","and you must build a detector that identifies where in the tissue cancer may reside.\n","\n","---\n","\n","### Part 0\n","Watch and follow video tutorial:\n","\n","https://youtu.be/Ms19kgK_D8w\n","\n","**TODO:**\n","\n","* Watch tutorial\n","\n","**DONE:**\n","\n","### Part 1\n","Implement a dense predictor\n","\n","In previous labs and lectures, we have talked about DNNs that classify an \n","entire image as a single class. Here, however, we are interested in a more nuanced classification: \n","given an input image, we would like to identify each pixel that is possibly cancerous. \n","That means that instead of a single output, your network should output an “image”, \n","where each output pixel of your network represents the probability that a pixel is cancerous.\n","\n","___\n","\n","Implement your network topology\n","\n","Use the “Deep Convolution U-Net” from this paper: [(U-Net: Convolutional Networks for Biomedical Image Segmentation)](https://arxiv.org/pdf/1505.04597.pdf) \n","\n","![(Figure 1)](https://lh3.googleusercontent.com/qnHiB3B2KRxC3NjiSDtY08_DgDGTDsHcO6PP53oNRuct-p2QXCR-gyLkDveO850F2tTAhIOPC5Ha06NP9xq1JPsVAHlQ5UXA5V-9zkUrJHGhP_MNHFoRGnjBz1vn1p8P2rMWhlAb6HQ=w2400)\n","\n","You should use existing pytorch functions (not your own Conv2D module), such as ```nn.Conv2d``` and ```nn.CrossEntropyLoss```;\n","you will also need the pytorch function ```torch.cat``` and ```nn.ConvTranspose2d```\n","\n","```torch.cat``` allows you to concatenate tensors.\n","```nn.ConvTranspose2d``` is the opposite of ```nn.Conv2d```.\n","It is used to bring an image from low res to higher res.\n","[This blog](https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0) should help you understand this function in detail.\n","\n","You can simplify the implementation of this lab by padding the feature maps as they pass through each convolution. This will make the concatenation process easier, though this is technically a departure from the cropping technique outlined in the orginal U-Net paper. \n","\n","Note that the simplest network you could implement (with all the desired properties)\n","is just a single convolution layer with two filters and no relu! \n","Why is that? (of course it wouldn't work very well!)\n","\n","**TODO:**\n","\n","* Understand the U-Net architecture\n","* Understand concatenation of inputs from multiple prior layers\n","* Understand ConvTranspose\n","* Answer Question / Reflect on simplest network with the desired properties\n","\n","**DONE:**\n","\n","\n","___\n","The intention of this lab is to learn how to make deep neural nets. We'll help you with the implementation of Dataset.\n","This code will download the dataset for you so that you are ready to use it and focus on network\n","implementation, losses and accuracies."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wQOefmcZVgTl","executionInfo":{"status":"ok","timestamp":1633102915848,"user_tz":360,"elapsed":14592,"user":{"displayName":"Cameron Greenwalt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16537017385880708948"}},"outputId":"c1682012-036f-40a3-e30a-4980efd97dfa"},"source":["!pip3 install torch\n","!pip3 install torchvision\n","!pip3 install tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torchvision import transforms, utils, datasets\n","from tqdm import tqdm\n","from torch.nn.parameter import Parameter\n","import pdb\n","import torchvision\n","import os\n","import gzip\n","import tarfile\n","import gc\n","from IPython.core.ultratb import AutoFormattedTB\n","__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n","\n","assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu102)\n","Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.0+cu102)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n"]}]},{"cell_type":"code","metadata":{"id":"Cj-RIAZiKvWN"},"source":["# print(torch.cuda.memory_allocated(0) / 1e9) # Print gpu memory in gigabytes\n","# # use model.cuda() NOT model = model.cuda()\n","# gc.collect() # force the garbage collector to clean up unused memory, call in the beginning of your loop\n","# # unhandled exceptions cause memory leaks\n","# # fix with\n","# try:\n","#   print('something')\n","# except:\n","#   __ITB__()\n","#\n","# # variables remaining in scope are another cause of memory leaks \\\n","#   # Fix this by creating a dummy function (e.g., scope()) that you call\n","#\n","#\n","# for obj in gc.get_objects():\n","#   if torch.is_tensor(obj):\n","#     print(obj.size())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"49quyvIJvTq8"},"source":["WARNING: You may run into an error that says \"RuntimeError: CUDA out of memory.\"\n","\n","In this case, the memory required for your batch is larger than what the GPU is capable of. You can solve this problem by adjusting the image size or the batch size and then restarting the runtime. "]},{"cell_type":"code","metadata":{"id":"Il_53HLSWPTY"},"source":["class CancerDataset(Dataset):\n","  def __init__(self, root, download=True, size=512, train=True):\n","    if download and not os.path.exists(os.path.join(root, 'cancer_data')):\n","      datasets.utils.download_url('http://liftothers.org/cancer_data.tar.gz', root, 'cancer_data.tar.gz', None)\n","      self.extract_gzip(os.path.join(root, 'cancer_data.tar.gz'))\n","      self.extract_tar(os.path.join(root, 'cancer_data.tar'))\n","    \n","    postfix = 'train' if train else 'test'\n","    root = os.path.join(root, 'cancer_data', 'cancer_data')\n","    self.dataset_folder = torchvision.datasets.ImageFolder(os.path.join(root, 'inputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n","    self.label_folder = torchvision.datasets.ImageFolder(os.path.join(root, 'outputs_' + postfix) ,transform = transforms.Compose([transforms.Resize(size),transforms.ToTensor()]))\n","\n","  @staticmethod\n","  def extract_gzip(gzip_path, remove_finished=False):\n","    print('Extracting {}'.format(gzip_path))\n","    with open(gzip_path.replace('.gz', ''), 'wb') as out_f, gzip.GzipFile(gzip_path) as zip_f:\n","      out_f.write(zip_f.read())\n","    if remove_finished:\n","      os.unlink(gzip_path)\n","  \n","  @staticmethod\n","  def extract_tar(tar_path):\n","    print('Untarring {}'.format(tar_path))\n","    z = tarfile.TarFile(tar_path)\n","    z.extractall(tar_path.replace('.tar', ''))\n","\n","  def __getitem__(self,index):\n","    img = self.dataset_folder[index]\n","    label = self.label_folder[index]\n","    return img[0],label[0][0]\n","  \n","  def __len__(self):\n","    return len(self.dataset_folder)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XPgrP88aOtfy"},"source":["# You'll probably want a function or something to test input / output sizes of the ConvTranspose2d layer\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jq22IyKanxo_"},"source":["# Since you will be using the output of one network in two places(convolution and maxpooling),\n","# you can't use nn.Sequential.\n","# Instead you will write up the network like normal variable assignment as the example shown below:\n","# You are welcome (and encouraged) to use the built-in batch normalization and dropout layer.\n","\n","# TODO: You need to change this to fit the UNet structure!!!\n","class CancerDetection(nn.Module):\n","  def __init__(self): # CancerDetection constructor\n","    # You always need to use super to call nn.Module's init function when creating a class that inherits nn.Module.\n","    super(CancerDetection, self).__init__()\n","\n","    # Dropout layer\n","    self.dropout = nn.Dropout(p=0.2)\n","    \n","    # Always initialize your layers in the init function.\n","    self.conv1 = nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 1)\n","    # self.batchnorm1 = nn.BatchNorm2d(64)\n","    self.relu2 = nn.ReLU()\n","\n","    self.conv3 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1, padding = 1)\n","    self.batchnorm2 = nn.BatchNorm2d(64)\n","    self.relu4 = nn.ReLU()\n","\n","    self.maxpool5 = nn.MaxPool2d((2,2))\n","\n","    self.conv6 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","    self.batchnorm3 = nn.BatchNorm2d(128)\n","    self.relu7 = nn.ReLU()\n","\n","    self.conv8 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n","    self.batchnorm4 = nn.BatchNorm2d(128)\n","    self.relu9 = nn.ReLU()\n","\n","    self.maxpool10 = nn.MaxPool2d((2,2))\n","\n","    self.conv11 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n","    self.batchnorm5 = nn.BatchNorm2d(256)\n","    self.relu12 = nn.ReLU()\n","\n","    self.conv13 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","    self.batchnorm6 = nn.BatchNorm2d(256)\n","    self.relu14 = nn.ReLU()\n","\n","    self.maxpool15 = nn.MaxPool2d((2,2))\n","\n","    self.conv16 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n","    self.batchnorm7 = nn.BatchNorm2d(512)\n","    self.relu17 = nn.ReLU()\n","\n","    self.conv18 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n","    self.batchnorm8 = nn.BatchNorm2d(512)\n","    self.relu19 = nn.ReLU()\n","\n","    self.maxpool20 = nn.MaxPool2d((2,2))\n","\n","    self.conv21 = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1)\n","    self.batchnorm9 = nn.BatchNorm2d(1024)\n","    self.relu22 = nn.ReLU()\n","\n","    self.conv23 = nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1)\n","    self.batchnorm10 = nn.BatchNorm2d(1024)\n","    self.relu24 = nn.ReLU()\n","\n","    self.convtrans25 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2) # doubles the dimensions\n","\n","    self.conv26 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)\n","    self.batchnorm11 = nn.BatchNorm2d(512)\n","    self.relu27 = nn.ReLU()\n","\n","    self.conv28 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n","    self.batchnorm12 = nn.BatchNorm2d(512)\n","    self.relu29 = nn.ReLU()\n","\n","    self.convtrans30 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2) # doubles the dimensions\n","\n","    self.conv31 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n","    self.batchnorm13 = nn.BatchNorm2d(256)\n","    self.relu32 = nn.ReLU()\n","\n","    self.conv33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","    self.batchnorm14 = nn.BatchNorm2d(256)\n","    self.relu34 = nn.ReLU()\n","\n","    self.convtrans35 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) # doubles the dimensions\n","\n","    self.conv36 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n","    self.batchnorm15 = nn.BatchNorm2d(128)\n","    self.relu37 = nn.ReLU()\n","\n","    self.conv38 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n","    self.batchnorm16 = nn.BatchNorm2d(128)\n","    self.relu39 = nn.ReLU()\n","\n","    self.convtrans40 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2) # doubles the dimensions\n","\n","    self.conv41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n","    self.batchnorm17 = nn.BatchNorm2d(64)\n","    self.relu42 = nn.ReLU()\n","\n","    self.conv43 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","    self.batchnorm18 = nn.BatchNorm2d(64)\n","    self.relu44 = nn.ReLU()\n","\n","    self.conv45 = nn.Conv2d(64, 2, kernel_size=3, padding=1)\n"," \n","  def forward(self, input): # nn.Module sets up a hook that calls forward when you \"call\" the module object: net(x) calls net.forward(x)\n","    conv1_out = self.conv1(input); del input\n","    batchnorm1_out = self.batchnorm1(conv1_out); del conv1_out\n","    dropout1_out = self.dropout(batchnorm1_out); del batchnorm1_out\n","    relu2_out = self.relu2(dropout1_out); del dropout1_out\n","\n","    conv3_out = self.conv3(relu2_out); del relu2_out\n","    batchnorm2_out = self.batchnorm2(conv3_out); del conv3_out\n","    dropout2_out = self.dropout(batchnorm2_out); del batchnorm2_out\n","    relu4_out = self.relu4(dropout2_out); del dropout2_out\n","\n","    maxpool5_out = self.maxpool5(relu4_out)\n","\n","    conv6_out = self.conv6(maxpool5_out); del maxpool5_out\n","    batchnorm3_out = self.batchnorm3(conv6_out); del conv6_out\n","    dropout3_out = self.dropout(batchnorm3_out); del batchnorm3_out\n","    relu7_out = self.relu7(dropout3_out); del dropout3_out\n","\n","    conv8_out = self.conv8(relu7_out); del relu7_out\n","    batchnorm4_out = self.batchnorm4(conv8_out); del conv8_out\n","    dropout4_out = self.dropout(batchnorm4_out); del batchnorm4_out\n","    relu9_out = self.relu9(dropout4_out); del dropout4_out\n","\n","    maxpool10_out = self.maxpool10(relu9_out)\n","\n","    conv11_out = self.conv11(maxpool10_out); del maxpool10_out\n","    batchnorm5_out = self.batchnorm5(conv11_out); del conv11_out\n","    dropout5_out = self.dropout(batchnorm5_out); del batchnorm5_out\n","    relu12_out = self.relu12(dropout5_out); del dropout5_out\n","\n","    conv13_out = self.conv13(relu12_out); del relu12_out\n","    batchnorm6_out = self.batchnorm6(conv13_out); del conv13_out\n","    dropout6_out = self.dropout(batchnorm6_out); del batchnorm6_out\n","    relu14_out = self.relu14(dropout6_out); del dropout6_out\n","\n","    maxpool15_out = self.maxpool15(relu14_out)\n","\n","    conv16_out = self.conv16(maxpool15_out); del maxpool15_out\n","    batchnorm7_out = self.batchnorm7(conv16_out); del conv16_out\n","    dropout7_out = self.dropout(batchnorm7_out); del batchnorm7_out\n","    relu17_out = self.relu17(dropout7_out); del dropout7_out\n","\n","    conv18_out = self.conv18(relu17_out); del relu17_out\n","    batchnorm8_out = self.batchnorm8(conv18_out); del conv18_out\n","    dropout8_out = self.dropout(batchnorm8_out); del batchnorm8_out\n","    relu19_out = self.relu19(dropout8_out); del dropout8_out\n","\n","    maxpool20_out = self.maxpool20(relu19_out)\n","\n","    conv21_out = self.conv21(maxpool20_out); del maxpool20_out\n","    batchnorm9_out = self.batchnorm9(conv21_out); del conv21_out\n","    dropout9_out = self.dropout(batchnorm9_out); del batchnorm9_out\n","    relu22_out = self.relu22(dropout9_out); del dropout9_out\n","\n","    conv23_out = self.conv23(relu22_out); del relu22_out\n","    batchnorm10_out = self.batchnorm10(conv23_out); del conv23_out\n","    dropout10_out = self.dropout(batchnorm10_out); del batchnorm10_out\n","    relu24_out = self.relu24(dropout10_out); del dropout10_out\n","\n","    convtrans25_out = self.convtrans25(relu24_out); del relu24_out\n","    convtrans25_out = torch.cat((convtrans25_out, relu19_out), dim=1); del relu19_out # ??\n","\n","    conv26_out = self.conv26(convtrans25_out); del convtrans25_out\n","    batchnorm11_out = self.batchnorm11(conv26_out); del conv26_out\n","    dropout11_out = self.dropout(batchnorm11_out); del batchnorm11_out\n","    relu27_out = self.relu27(dropout11_out); del dropout11_out\n","\n","    conv28_out = self.conv28(relu27_out); del relu27_out\n","    batchnorm12_out = self.batchnorm12(conv28_out); del conv28_out\n","    dropout12_out = self.dropout(batchnorm12_out); del batchnorm12_out\n","    relu29_out = self.relu29(dropout12_out); del dropout12_out\n","\n","    convtrans30_out = self.convtrans30(relu29_out); del relu29_out\n","    convtrans30_out = torch.cat((convtrans30_out, relu14_out), dim=1); del relu14_out\n","\n","    conv31_out = self.conv31(convtrans30_out); del convtrans30_out\n","    batchnorm13_out = self.batchnorm13(conv31_out); del conv31_out\n","    dropout13_out = self.dropout(batchnorm13_out); del batchnorm13_out\n","    relu32_out = self.relu32(dropout13_out); del dropout13_out\n","\n","    conv33_out = self.conv33(relu32_out); del relu32_out\n","    batchnorm14_out = self.batchnorm14(conv33_out); del conv33_out\n","    dropout14_out = self.dropout(batchnorm14_out); del batchnorm14_out\n","    relu34_out = self.relu34(dropout14_out); del dropout14_out\n","\n","    convtrans35_out = self.convtrans35(relu34_out); del relu34_out\n","    convtrans35_out = torch.cat((convtrans35_out, relu9_out), dim=1); del relu9_out\n","\n","    conv36_out = self.conv36(convtrans35_out); del convtrans35_out\n","    batchnorm15_out = self.batchnorm15(conv36_out); del conv36_out\n","    dropout15_out = self.dropout(batchnorm15_out); del batchnorm15_out\n","    relu37_out = self.relu37(dropout15_out); del dropout15_out\n","\n","    conv38_out = self.conv38(relu37_out); del relu37_out\n","    batchnorm16_out = self.batchnorm16(conv38_out); del conv38_out\n","    dropout16_out = self.dropout(batchnorm16_out); del batchnorm16_out\n","    relu39_out = self.relu39(dropout16_out); del dropout16_out\n","\n","    convtrans40_out = self.convtrans40(relu39_out); del relu39_out\n","    convtrans40_out = torch.cat((convtrans40_out, relu4_out), dim=1); del relu4_out\n","\n","    conv41_out = self.conv41(convtrans40_out); del convtrans40_out\n","    batchnorm17_out = self.batchnorm17(conv41_out); del conv41_out\n","    dropout17_out = self.dropout(batchnorm17_out); del batchnorm17_out\n","    relu42_out = self.relu42(dropout17_out); del dropout17_out\n","\n","    conv43_out = self.conv43(relu42_out); del relu42_out\n","    batchnorm18_out = self.batchnorm18(conv43_out); del conv43_out\n","    dropout17_out = self.dropout(batchnorm18_out); del batchnorm18_out\n","    relu44_out = self.relu44(dropout17_out); del dropout17_out\n","\n","    conv45_out = self.conv45(relu44_out); del relu44_out\n","\n","    return conv45_out\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NAjagHCdGNAh"},"source":["# Create your datasets, dataloaders and neural network as you have before\n","gc.collect()\n","train_dataset = CancerDataset('cancer', train=True)\n","val_dataset = CancerDataset('cancer', train=False)\n","\n","train_loader = DataLoader(train_dataset, \n","                          batch_size=3,\n","                          shuffle=True,\n","                          pin_memory=True)\n","val_loader = DataLoader(val_dataset,\n","                        batch_size=3,\n","                        shuffle=True,\n","                        pin_memory=True)\n","\n","model = CancerDetection()\n","model.cuda()\n","objective = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","num_epochs = 5\n","# Tip: Pass shuffle=True to constructor for training dataloader to automatically shuffle training data each epoch. You can shuffle the validation loader, but you don't need to."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zrcUuaVDTVYU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633104196978,"user_tz":360,"elapsed":8,"user":{"displayName":"Cameron Greenwalt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16537017385880708948"}},"outputId":"5bde5f8e-7740-4319-c795-95dc1a6e5ccf"},"source":["# Print number of parameters in model\n","total = 0\n","for p in model.parameters():\n","  size = 1\n","  for s in p.size():\n","    size = size * s\n","  total += size\n","print (total)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["31044610\n"]}]},{"cell_type":"code","metadata":{"id":"RkieTbwlYWPS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6048ea69-00f4-4860-a9ea-fec6ff17ce4a"},"source":["# This is what was talked about in the video for memory management\n","\n","train_losses = []\n","validation_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","test_images = []\n","\n","def scope():\n","  try:\n","    #your code for calling dataset and dataloader\n","    gc.collect()\n","    print(torch.cuda.memory_allocated() / 1e9)\n","\n","    for epoch in range(num_epochs):\n","\n","      losses = []\n","      accuracies = []\n","      loop = tqdm(total=len(train_loader), position=0, leave=False)\n","\n","      for batch, (x, y_truth) in enumerate(train_loader):\n","        x, y_truth = x.cuda(), y_truth.cuda()\n","        optimizer.zero_grad()\n","\n","        y_hat = model(x)\n","        loss = objective(y_hat, y_truth.long()) # PyTorch expects the y_truth to be long\n","        losses.append(loss.item())\n","        loss.backward()\n","\n","        accuracy = (torch.softmax(y_hat, 1).argmax(1) == y_truth).float().mean()\n","        accuracies.append(accuracy.item())\n","\n","        optimizer.step()\n","\n","        loop.set_description('epoch:{} loss:{:.4f}'.format(epoch, loss.item()))\n","        loop.update(1)\n","\n","      # Record train and validation loss information for epoch\n","      train_losses.append(np.mean(losses))\n","      val_loss = np.mean([objective(model( x.cuda() ), y.cuda().long()).item()\n","                      for x, y in val_loader])\n","      validation_losses.append((len(train_losses) - 1, val_loss))\n","\n","      # Record train and validation accuracy information for each epoch\n","      train_accuracies.append(np.mean(accuracies))\n","      val_accuracy = np.mean( [(torch.softmax(model( x.cuda() ), 1).argmax(1) == y.cuda()).float().mean().item()\n","                              for x, y in val_loader] )\n","      val_accuracies.append((len(train_accuracies) - 1, val_accuracy))\n","\n","      if epoch % 1 == 0:\n","        # Make image of testing prediction every 5 epochs\n","        # Get the test image\n","        x_test, y_test = val_dataset[172]\n","\n","        # Put data on GPU\n","        x_test, y_test = x_test.cuda(), y_test.cuda()\n","\n","        # Make data in right dimensions (need batch size 1)\n","        x_test = x_test.unsqueeze(0)\n","        y_test = y_test.unsqueeze(0)\n","\n","        # Turn prediction into BW image\n","        out_img = (torch.softmax(model(x_test), 1).argmax(1) ) \\\n","              .squeeze(0) \\\n","              .cpu()  \\\n","              .detach() \\\n","              .numpy()\n","        test_images.append(out_img)\n","      \n","      loop.close()\n","      \n","  except:\n","    __ITB__()\n","    \n","scope()"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.126207488\n"]},{"output_type":"stream","name":"stderr","text":["epoch:3 loss:0.1794:  57%|█████▋    | 255/448 [10:50<08:12,  2.55s/it]"]}]},{"cell_type":"markdown","metadata":{"id":"CZ062Jv1jIIu"},"source":["\n","___\n","\n","### Part 2\n","\n","Plot performance over time\n","\n","Please generate two plots:\n","\n"," One that shows loss on the training and validation set as a function of training time. \n","\n"," One that shows accuracy on the training and validation set as a function of training time. \n","\n"," Make sure your axes are labeled!\n","\n","**TODO:**\n","\n","* Plot training/validation loss as function of training time (not epochs)\n","* Plot training/validation accuracy as function of training time (not epochs)\n","\n","**DONE:**\n","\n"]},{"cell_type":"code","metadata":{"id":"mTg1jyIsYVZN","executionInfo":{"status":"error","timestamp":1633110026808,"user_tz":360,"elapsed":211,"user":{"displayName":"Cameron Greenwalt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16537017385880708948"}},"outputId":"0b16ee72-6d6b-41b3-e6a8-cab7510f8a14","colab":{"base_uri":"https://localhost:8080/","height":243}},"source":["# Plot training and validation loss for uniform initialization\n","a, b = zip(*validation_losses)\n","f1 = plt.figure()\n","ax1 = f1.add_subplot(111)\n","ax1.plot(train_losses, label='Training loss')\n","ax1.plot(a, b, label='Validation loss')\n","plt.title('Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","# Plot training and validation accuracy for uniform initialization\n","a, b = zip(*val_accuracies)\n","f2 = plt.figure()\n","ax2 = f2.add_subplot(111)\n","ax2.plot(train_accuracies, label='Training accuracy')\n","ax2.plot(a, b, label='Validation accuracy')\n","ax2.axhline(y=0.85, color='black', linestyle='dashed', label='0.85')\n","plt.title('Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e64fc9c67b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot training and validation loss for uniform initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalidation_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'validation_losses' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"XS3ljWCAbMAa"},"source":["**NOTE:**\n","\n","Guessing that the pixel is not cancerous every single time will give you an accuracy of ~ 85%.\n","Your trained network should be able to do better than that (but you will not be graded on accuracy).\n","This is the result I got after 1 hour or training.\n","\n","![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=400&tok=d23e0b&media=cs501r_f2016:training_accuracy.png)\n","![](http://liftothers.org/dokuwiki/lib/exe/fetch.php?w=400&tok=bb8e3c&media=cs501r_f2016:training_loss.png)"]},{"cell_type":"markdown","metadata":{"id":"S4s92S2_jQOG"},"source":["___\n","\n","### Part 3\n","\n","Generate at least 5 predictions on the pos_test_000072.png image and display them as images. These predictions should be made at a reasonable interval (e.g. every epoch). \n","\n","You can load this image from the file pos_test_000072.png, or you can get it from the dataset object. It is item 172 of the validation dataset.\n","You can print both the data instance (x) and the ground-truth label (y_hat) to see how well your network predicts on that instance.\n","\n","To do this, calculate the output of your trained network on the pos_test_000072.png image,\n","then make a hard decision (cancerous/not-cancerous) for each pixel.\n","The resulting image should be black-and-white, where white pixels represent areas\n","the network considers cancerous.\n","\n","**TODO:**\n","\n","**DONE:**\n","\n"]},{"cell_type":"code","metadata":{"id":"XXfG3wClh8an","outputId":"8e9534a5-0d55-4e47-8051-829929178512"},"source":["# Predictions at 5 epoch intervals\n","import matplotlib.pyplot as plt\n","\n","for image in test_images:\n","  f1 = plt.figure()\n","  ax1 = f1.add_subplot(111)\n","  ax1.imshow(image, cmap='Greys_r')\n","  plt.show()"],"execution_count":null,"outputs":[{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"},{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjZ0lEQVR4nO3de3QU9d0/8PdncyHGpEBQJMSEQE+wh6rhVqhQPZiCQKWYoyJeKKgEWrVWAaEYeRAvT6seax8qjULlqXK8HdBaUXr5cdEAAiJUEUIgJAohIdxiEnLdZHfevz+yybNhNruTZDe7C5/XOZ+zu7Mzs9+Z7L4z9xGSUEopd7ZgN0ApFXo0GJRSJhoMSikTDQallIkGg1LKRINBKWUSsGAQkUkiclhECkVkcaA+RynlfxKI4xhEJAJAAYAJAEoAfAHgLpIH/f5hSim/C9QSwygAhSS/IdkI4F0AtwTos5RSfhYZoPEmATju9roEwOj2ehYRPfzSz0QEPXr0QK9evVBeXo6mpiYAQFxcHFJTU9GjR48Oja+xsREHDx6E0+kMRHNV9zhL8nIrPQYqGMRDtzY/fhGZC2BugD7/okcScXFx2LVrF3bu3ImZM2eiqakJNTU1mDNnDqZMmYL+/fsjJibG0vhOnjyJIUOGoKKiIsAtVwF0zHKfJP1eAK4D8G+3148DeNxL/9TyX9lsNmZnZ/PUqVMkSbvdzokTJ7a+n56ezvHjx/P666/npk2b6IthGHziiSeCPl1aXa49ln/DAQqGSADfABgIIBrAPgA/1GDonrr11ltZX1/f5se9detW2mw29u7dm9988w3r6+uZm5vLzz//3Gcw1NfX8+qrrw76dGl1uYIbDK4f+8/QvGeiCMATPvoN9gy7IOqSSy7hX//6V9bW1pp+3CdOnGB8fDxvvvlmNjY2+gwDd/v37w/6tGn5pYIfDB0MkWDPsLCvvn37ct26dXQ6nW1+1Ha7nSTpcDhYXl7Ourq6DoVCVVUVp0yZEvTp0/JLaTBcLOW+PcEwjDY/6vr6ej722GOm1QqrGhsbOW3atKBPo5bfSoPhYqkhQ4awqKio3R/3uXPnOhUKJFlWVsY+ffoEfRq1/FaWgyFQuytVAERERCA5ORnDhw/HjTfeCAAYNWoUBg0a1O4w8fHxnf68urq6Dh/voC4MATkkusON0AOc2hARXHXVVRgwYABGjx6NjIwMAEBMTAzS09MRGRmJyMjuyfS8vDwsXLgQ27ZtQ01NTbd8pgqYvSRHWukxJILhiiuu4LRp0/DBBx/gxIkTHvuJiYmBiOB73/sebrvtNpDE3/72N5SXl8PhcHRzi/0nOjq69b/yT37yE0yaNAk///nP0bdvX8TGxkLE07Fi3Yskjh07hjlz5mDTpk3Bbo7qPMvBEPTtCyQxYsSI1nXalJSUNutFSUlJvO+++3jo0CEWFhby7NmzNAyDhmHw3Llz3LRpU9jtY4+MjGRaWhqfffZZ7tu3j5WVlaysrGzdg+BJdXV1hzYiGobBqqoqHjp0yPIwvlRUVHDJkiWMiooK+jzU6lSF18bHlmAgyc8++4xZWVncvn07P/vsM1ZVVdHhcHj9whYXFzMhISHYM91yDRkyhHV1da17EQzDYHFxcbsbCsvLyzlw4EDecccdbGpq8vHzbT5mYdGiRUxISOC9997rs/+OcDqdXLlypYZDeFb4BkNjY6PX/5ztfVmzs7ODPdMtV0xMDN97773WH7nD4eCIESN4/fXXs6qqyjR9W7ZsYUJCAkeOHNlm3jQ2NvLLL79ssyRRWVnJfv36EQATEhJ44MCBDs1Lq/P717/+ddDno1aHy3IwhNwVnKKiohAdHd2hYWw2G371q191eLhgaWhowIwZM/Doo4/C4XDAZrMhMzMTI0aMgGEYpv5vuOEGHD16FJ988gmioqJauy9fvhwZGRkoKSlp7UYSjY2NAID7778fQ4YM8Xv7bTab1z0h6gJgNUECWe5LDJ1lt9uZlZUV7ETuUE2bNq318GSn02k6atGXmpoa7t27t83qhWEYzM3N5d13382ysrIuz9f2lJSUMDIyMujzUKtDFb6rEl1RVlbGyy67LNgz31JFREQwPz/fL9MdDGVlZRoM4VfhuyrRFREREYiIiAh2MywREfTs2TPYzbCMJOx2e7CbobpJSBz5aLfb4XA4UFxcjIaGBgBAbW0tXnvtNRiGgYceegjXXnstbDbvOdbQ0NA6vPIvwzDQ1NSkR0JeJEIiGPLy8nDdddfhwIEDHn/YmzdvxqFDh3xuXExOTsbGjRvx8ccfIz8/H+vWrQtUk7usZ8+ePpdu7HY7DMNAVFRUtx3peD6SaGhoQEREBOLi4lq7t2zgVBcoq+scgSz4WDcaPXp0h68h8PLLLwd7fa7d6tevH48dO+Z1Y+P+/fs5atQo9uvXjzNnzuS6det48uRJnjx5kqdPn+7whsoW9fX1prMwvfnuu++YmprKSZMm8fDhwySbN5QuXrw46PNRq8MVXhsfvU1MZGQkP/roI8tfZMMwWFZWxoyMjGD/EdqtAQMG+Dzr8ezZs+zfv3/rMCLCqKgoRkVFMTY2lrfeeivXrVvX5kApX7799lsuWLDAY8g6HI7W8TQ0NHDfvn1csmQJt2/f3now09ChQ/n3v/+dr7zyCmNiYoI+H7U6XOEVDAkJCZw7dy779evHa665hk8//TSnT5/O9PR0vvfeez6PfDQMg3l5eXznnXc4b94802HVoVi7du3y+V8/Ly+PvXv39jqewYMH89lnn7W0BNHQ0MCamhpT99zcXI4fP57Lli3jm2++yTFjxlBEWi8FF+x5peW3Cq9gGD58OB0OB+vq6lqP4mtsbLR8bsDOnTvD6pBoALznnnu4bt06NjQ0tDtdTqeT8+bNa3ccgwYN4ujRo/mLX/zCa3g6nU6P87K2tpZr165lUlJS0OeHVrdUeAVDZ49jaGho4JIlS9irV69gz3BL1adPH86aNYvz589nWVkZjxw5Yuk8kEsvvdQ0rkmTJrGqqsrSgVFHjx5tc8yEYRh85plnePXVV9N1yrvWxVEXZjAYhsGamhqWlpZyzZo1vP/++8Pqix0XF8djx46ZpsvhcLT7425oaOC4ceNaxxEdHc3s7GyP51RYVV1dzbS0tKDPD61urwsvGCoqKpibm8uUlBTGx8cHewZ3qkSEH374oWnaampqvK5S7Ny5k2PHjmV8fDxfe+01S9sT7HY7V65c6XEVIi8vj9HR0UGfH1rdXuEfDHa7nXv27OHDDz/MzMxMJicn02azBXvGdrkGDx7M4uJinz/s8zkcDh4/ftxSKDidTq5evZrJyck8ffo0yebdlG+99RYzMzPDYuOsVkAqvIJh+PDhrWFw4sQJrl69mnPmzLkggsBTDR48mP/617/Y0NDQoWMKrDp69Ch/8IMfcOHCha2naW/atOmCnZ9aliu8gqFnz56cP38+J0+ezLi4uGDPvG6pqKgoZmRkcM+ePX4PBqfTyYaGhjYbNpcsWRL0adYKeoVXMITADAta3XLLLW1Om66qqur0fSDaYxgGf/vb3wZ9WrWCXhfn2ZXh6D//+Q9qa2tbXy9cuBCvv/56h8Zx7tw5bN26td33T506hXfffbezTVQXIQ2GIKuursa3337b+nrgwIEYN26c5eENw8ALL7yAl19+2fReU1MT/vCHP2DEiBE4dsz6HdCVCvpqxMW+KgGAY8eO9XgjWl+rBy0HN+Xm5nLr1q2m93fs2MGIiIigT59WyJRuY/BU6enpXLVqFV966aWQu9LTzJkzLZ9BWl9fzz/+8Y/ct29fu/1UVVUxNTU16NOlFVKlwXB+DR06tPUaiIZhsKioiA888ACnTp3KsWPHMioqiuPHj+eUKVPYs2fPbv+jxcTEcMOGDZZ2X86fP582m407duzw+H5tbS3vuuuuYH8JtUKvNBiA5sOHBw0axJUrV3o9zdlut7O0tLT11OPPP/+cN954Y7f/4WJjY/nRRx/5DIc9e/Zww4YNpr0XDoeDBQUFvOOOO4L9BdQKzdJg6NmzJ7ds2dLhdfcWp06d8nqn54SEBA4aNMjv7b700kv54Ycfej1E+nwtgTB79uyL5jgQrU7VxRsMAwcOZE5ODgsLC03/eVs22FlhGAY/+ugjj58xe/ZslpaWsra2ljNnzvT7HzAiIoLjxo3jJ598wrNnz7ZuZGyplrMl8/Pz+f7773P69Om85JJLgv2l0wr9ujiDISIiot17NdbX13PevHkcN24c33//fdbV1fkMh4KCAtMpzyNGjGizCF9bW8shQ4YE7I+ZmJjIMWPGtClPp2FraVko/wUDgP8FcBrAAbduCQA2Ajjieuzt9t7jAAoBHAYwsTuDYdCgQR63JTidTj788MOtp2jbbDYOGzbM58lMDoeD+fn5HDBgAIHm06Y93fLtnXfe0fMQtMKh/BoMNwAYjrbB8AKAxa7niwE873o+BMA+AD0ADARQBCCiO4IhPj6eM2bMYGVlpemHW1RU5HHde/DgwTx+/LjXcCDJN998kyLCMWPGeDxcecuWLXrzFa1wKP+uSgBIRdtgOAwg0fU8EcBh/t/SwuNu/f0bwHWBDoZevXoxNze33a35K1eubHfYrKwsnxsoa2pqOGvWLJ48edLj+zk5OcH+g2tpWamAB0Plee9XuB5XAJjh1n01gNvbGedcAHtc1aUJnj9/vtddfNOmTWt3WJvNxqFDh/pccmjv9vNnzpzhNddcE+w/uJaWlQraSVTioRs99UhyFcmRJEd25QNtNhvuueceiHj6aN8Mw8BXX32Fn/70pzh+/Hi7/Xm64cvBgweRnp6O/fv3d+qzlQpVnQ2GUyKSCACux9Ou7iUAkt36uxLAic43z7fU1FSkpaV1eTwFBQW46aabUFxcbKn/+vp6TJ06FSdOBHTylAqKzgbDegCzXM9nAfjQrfudItJDRAYCSAOw29fIEhMT8eKLLyImJqbDDbn22msRHx/vtZ/LL7/c0rgOHTqEhQsXWurXMAxUVlZa6lepcOMzGETkHQA7AVwlIiUiMhvAcwAmiMgRABNcr0EyD8BaAAcB/AvAQySdvj6jf//+ePTRR3Hbbbc1N8pmw7Bhw3DfffchOTm5y/dtXLBgQZv7Lnpj9aa4hmF0pUlKhTSfvziSd7Xz1k/b6f+/Afx3RxsSERGBV199Fb169cJNN92ECRMm4JJLLkF1dTV2796N22+/3eN/6IaGBjidTq83iE1JSUFqaioOHDjQ0Wa1a/v27aioqPDb+JQKKVa3Ugay3K8S7ekGLIZhMDc31+Nl46Ojo9vcTMWTjlzabOrUqV7HRTYfMHXvvfd2y5bkpKQkjhs3jhMmTODatWu5evVqDh48ONhbt7XCs8LrkGgr95UwDINTpkzxOMEzZsxgfX19u7sUSbK0tNTSfRhvu+02n21xOBxMT08P6B/RZrMxOzubFRUVNAyjze7YmpoaFhQUcPv27Zw0aVJY3XRHK6gVXsFwzTXX+PwxkmR+fj6Tk5NNE2yz2Tho0CD+13/9V7vDOp1Orlu3zuvJRgMGDGBBQYHXNjidThYWFvKKK64I6B9x/Pjxli7cUldXx6ysLMbGxgb7S6cV+hVewZCenu7zB9AiPz+/ze3hWyopKYlnzpzxOqxhGHzvvfc8hoPNZuP777/vdXin08lXXnkl4CcxRUVFcdu2bZbnSVNTEzdt2qQnV2n5qvAKho7e1HbHjh2MiopifHx86zkK6enpXlclWhiGwX/84x8cNmxYmxOfEhMTvV7MxeFw8M9//jOjoqI6/AeJjIzs0I82KSmpw5eQNwyDmzdvDtvb92l1S13YwVBVVcWtW7eysrKSr7/+OtPS0nj77bf7vHO0u7q6Oq5fv56zZ8/m/Pnz2739W35+PlesWMFFixZ1KBTi4uKYlpbG+fPnMzc3l2VlZVy2bJmle0bGx8ezsLCwQ/OE1HDQ8lkXdjCc/2Ow2+2WL6R6Pm93mnY6nZ26xNvUqVNZUFBAu93eZtxOp5PPP/+812FtNhtvuOGGTt+hyjAMrl27NthfQK3QrIsnGALJ6XRyzJgx7c7o/v37c/ny5czOzmbfvn3Zp08fLliwgNXV1e2O88CBA+1uKLTZbFy6dKmlVSJv6uvrec899wT7S6gVeqXB4A92u51Dhw71OJMjIiK4YcMGks3/paurq3nu3Dmfl46rr69nUlKSx3HOnz+/00s+59Ob2Gp5KMvB0LVjjS9Q1dXVrWdd7tu3z2M/P/rRj5CRkQEAEBHLh1x7YrPZkJGRgbvuugtRUVGdHo+7H//4x7j88stx6tQpv4xPXVw0GM7jdDqRlZWFXbt2obq6unmxyk1sbCymT5+O3/3udx0+6cswDJSWlqKurg4AEB0djaSkJCxatAhZWVldPifEXWdPQ1cKAIK+GsF2ViWqqqq4fPlyfvrpp35ZtLbC4XBw165d7e59mDFjBg8dOtShvR8t6urquGjRotajL+Pi4rhp06ZOX97el9ra2oAfhKUVdhX+2xjWrFlDAExJSeGbb75p+bLvneVwOJiTk9PukZH3339/l25PX11dzSVLlrSOz9dVp7pKg0HLQ4V/MHzxxRetX+yoqKhO775rj91uZ11dHevq6lhbW+v14KWIiAju37+/y59ZWFjI5ORkZmZmsqSkxA9T0b5PP/1Ub2irdX6F/8bHESNGYP/+/Zg+fTo++eQTvPXWW7j22ms7vXHObrcjJyendf0+Nze3zWnYZ86cgcPh8DjsVVddhcGDB3fqc92lpqbi0KFDiIyMRHR0dJfH501BQQGcTp+XwlDKM6sJEsjytruysrKSDzzwANeuXdvlRfmEhIROJW1cXBxfeuklr0cjGobBioqKTrfPnxwOB0ePHh3s/05aoVfhvyrhbxUVFZZOu/ZWTz31lMdxHzlyhMuXL2daWhrLy8vbvOdwOHj8+HHa7faAT2OLAwcOMCYmJthfQq3QKw0Gd/n5+bz11lu7fN2CjIwMHj16lGVlZa1HJ9bW1vLqq68m0Lwt5Omnn2ZxcTFPnDjBoqIi/uY3v+GAAQO8nqDlTw6Hg3fffXewv4BaoVkaDO6ys7O9zrCObKSLjIxkdHQ0p0+fzp07d3LWrFlt3rfZbIyMjGwtEWFcXByLiooCNn2GYbCpqYlff/01p0+frhsdtdqr8AqGYcOGBegn06y0tJQpKSkeZ9bEiRO5ceNGPvjggxw/fjwBUEQ4adIkv1zf4LHHHmNmZiaXLl1q2uXa2NjI6urqDh8XYRgGa2truXPnTr711ltcsWIFhw0bpqsPWr4qvIKhR48e7d6l2l9WrVrlcWatWLGCZPMieG1tLTdu3MjNmzezrq6Ob7/9dpf/GIsXL2ZDQ0ObE6Oampp4/Phxzp07lwkJCXzyySdbVzUMw+CpU6dYUlLC7777jtXV1czLy+OaNWtYUlLC48eP88UXX2RKSkqnrg2hdVFXeO2utNvtmDBhAr7++mv06tXL7+MnCZvN85Xyi4qKWq8yHRsbi/Hjx+O7777D119/7ZerQK9cuRLXXXcd8vPzsWvXLgBATU0NPv3009bdo0899RTWrFmD9PR0kMSWLVtQV1eHyy67DD179sTRo0fR1NTUOg26G1IFXEf/uwei4Eq0OXPmdGmXpCcVFRVcunQp+/bt6zFFIyMj+cUXX7T2f+rUKQ4bNixgqT158mQuXbqUiYmJwf7voXXxVXitSrQ0XESYlZXFs2fP+iUUnE4n582b53OG5ebmkmxenTh/Y6K/KiYmhsuXL2d1dTUNw2B5eTkfffRRr8P06dOH2dnZ3LhxI2+++eZgf6m0wr/CMxhaKikpyefVmn1pbGxkTk6OpfXwzMxMVlVV8auvvvL7BrykpCTm5OSwuLjYdG5ERUUFx44daxomNjaWKSkpzMvLax2mtraW//znP5mfn88nn3ySvXr1CvaXTCv8KryDAWg+k7EjVzKqr69nZWUlKysruWPHDk6ePLlDG+eSk5O7fADU+dW7d2+fG1X/9Kc/tfYfFRXF7OxslpaWej3uwTAMPvvss8H+kmmFX4VXMPTp04eTJ0/m97///daJiI2N5bJly3jmzBnTDVfO/5Fs27aNY8eOZXx8POPj40Nma31aWprPIx5LS0tb2/v0009bPovUPVC0tCxWeO2VSElJwYYNG1BRUYHMzExs27YNdXV1WLZsGXJycjBs2DD0798fs2bNajNcZWUl/vKXvyA3Nxc1NTVBan3XtFycpV+/fnjwwQfb3XvizjAMHDlyJNBNUxezYC8tkG2PfCwvL/e43h2OlZqa6vNCLOXl5bz99tu5e/duy9dnyMvL020MWp2p8FqVOP+Q6PLycq9XZw6XEhGuXLmSDQ0NrT/681eLWg5ntsowDN1DodXZCq9VifMlJCRgzpw52LFjR7Cb0iUk8ctf/hKvvvoqbrnlFiQlJeHtt9/G0KFDMXjwYEycOBEDBw7s0LUe6+rqUFRUFMBWKwUEfWnB0xIDSX7wwQcX/OXP582b16FL1jmdznYP7dbSslDhvSpBkkePHmVcXFywZ2RAKzY2lo888gjz8/PZ0NBAp9PpNSg2bdpk6RZ3WlrtlP+CAUAygE8A5APIA/CIq3sCgI0Ajrgee7sN8ziAQgCHAUzsaDA4HA6uWLEi2DOx20pEmJ6ezjFjxvCGG27gunXr2my0dDqdLC4ubr3ug5ZWJ8uvwZAIYLjreTyAAgBDALwAYLGr+2IAz7ueDwGwD0APAAMBFAGIsBoMNTU1fOaZZy7qG7PabDamp6fz97//PZ977jkuWLCg05el09Jyq8CtSgD4EMAENC8NJLqFx2G3pYXH3fr/N4DrfAWD0+nkgQMHOGLEiC5faUlLS8tjWQ4G30fTuBGRVADDAHwO4AqSZQDgeuzr6i0JwHG3wUpc3bx65513MHz4cOzdu7clUJRSQWJ5P5mIxAF4H8CjJM95uQWapzdMv3QRmQtgLgDEx8cjKysLjY2NVpujlAogS8EgIlFoDoW3SP7N1fmUiCSSLBORRACnXd1L0LzBssWVAE6cP06SqwCsco1fFxGUCiE+VyWkedFgNYB8ki+5vbUewCzX81lo3vbQ0v1OEekhIgMBpAHY7b8mK6UCzcoSw1gAvwCwX0S+cnXLBvAcgLUiMhtAMYBpAEAyT0TWAjgIwAHgIZJ6LTKlwoiEwoY+XZVQqlvsJTnSSo8d2iuhlLo4aDAopUw0GJRSJhoMSikTDQallIkGg1LKRINBKWWiwaCUMtFgUEqZaDAopUw0GJRSJhoMSikTDQallIkGg1LKRINBKWWiwaCUMtFgUEqZaDAopUw0GJRSJhoMSikTDQallIkGg1LKRINBKWWiwaCUMtFgUEqZaDAopUw0GJRSJhoMSikTDQallIkGg1LKRINBKWWiwaCUMtFgUEqZaDAopUx8BoOIxIjIbhHZJyJ5IvKUq3uCiGwUkSOux95uwzwuIoUiclhEJgZyApRS/mdlicEOIINkOoChACaJyI8BLAawmWQagM2u1xCRIQDuBPBDAJMA5IhIRADarpQKEJ/BwGY1rpdRriKAWwC84er+BoBM1/NbALxL0k7yWwCFAEb5s9FKqcCytI1BRCJE5CsApwFsJPk5gCtIlgGA67Gvq/ckAMfdBi9xdTt/nHNFZI+I7OlC+5VSAWApGEg6SQ4FcCWAUSJytZfexdMoPIxzFcmRJEdaaqlSqtt0aK8EyUoAn6J528EpEUkEANfjaVdvJQCS3Qa7EsCJrjZUKdV9rOyVuFxEermeXwJgPIBDANYDmOXqbRaAD13P1wO4U0R6iMhAAGkAdvu53UqpAIq00E8igDdcexZsANaS/FhEdgJYKyKzARQDmAYAJPNEZC2AgwAcAB4i6QxM85VSgSCkafW/+xshEvxGKHXh22t1m54e+aiUMtFgUEqZaDAopUw0GJRSJhoMSikTDQallIkGg1LKRINBKWWiwaCUMtFgUEqZaDAopUw0GJRSJhoMSikTDQallIkGg1LKRINBKWWiwaCUMtFgUEqZaDAopUw0GJRSJhoMSikTDQallIkGg1LKRINBKWWiwaCUMtFgUEqZaDAopUw0GJRSJhoMSikTDQallIkGg1LKRINBKWWiwaCUMrEcDCISISJfisjHrtcJIrJRRI64Hnu79fu4iBSKyGERmRiIhiulAqcjSwyPAMh3e70YwGaSaQA2u15DRIYAuBPADwFMApAjIhH+aa5SqjtYCgYRuRLAzQBec+t8C4A3XM/fAJDp1v1dknaS3wIoBDDKL61VSnULq0sM/wNgEQDDrdsVJMsAwPXY19U9CcBxt/5KXN3aEJG5IrJHRPZ0tNFKqcDyGQwiMgXAaZJ7LY5TPHSjqQO5iuRIkiMtjlcp1U0iLfQzFsBUEfkZgBgA3xORNwGcEpFEkmUikgjgtKv/EgDJbsNfCeCEPxutlAosn0sMJB8neSXJVDRvVNxCcgaA9QBmuXqbBeBD1/P1AO4UkR4iMhBAGoDdfm+5UipgrCwxtOc5AGtFZDaAYgDTAIBknoisBXAQgAPAQySdXW6pUqrbCGla/e/+RogEvxFKXfj2Wt2mp0c+KqVMNBiUUiYaDEopEw0GpZSJBoNSykSDQSllosGglDLRYFBKmWgwKKVMNBiUUiYaDEopEw0GpZSJBoNSykSDQSllosGglDLRYFBKmWgwKKVMNBiUUiYaDEopEw0GpZSJBoNSykSDQSllosGglDLRYFBKmWgwKKVMNBiUUiYaDEopEw0GpZSJBoNSykSDQSllosGglDLRYFBKmWgwKKVMNBiUUiaWgkFEjorIfhH5SkT2uLoliMhGETnieuzt1v/jIlIoIodFZGKgGq+UCoyOLDHcSHIoyZGu14sBbCaZBmCz6zVEZAiAOwH8EMAkADkiEuHHNiulAqwrqxK3AHjD9fwNAJlu3d8laSf5LYBCAKO68DlKqW5mNRgI4P+JyF4RmevqdgXJMgBwPfZ1dU8CcNxt2BJXtzZEZK6I7GlZNVFKhY5Ii/2NJXlCRPoC2Cgih7z0Kx660dSBXAVgFQCIiOl9pVTwWFpiIHnC9XgawAdoXjU4JSKJAOB6PO3qvQRAstvgVwI44a8GK6UCz2cwiMilIhLf8hzATQAOAFgPYJart1kAPnQ9Xw/gThHpISIDAaQB2O3vhiulAsfKqsQVAD4QkZb+3yb5LxH5AsBaEZkNoBjANAAgmSciawEcBOAA8BBJZ0Bar5QKCCGDv3ovImcA1AI4G+y2WHAZtJ3+Fi5tDZd2Ap7bOoDk5VYGDolgAAAR2eN2jETI0nb6X7i0NVzaCXS9rXpItFLKRINBKWUSSsGwKtgNsEjb6X/h0tZwaSfQxbaGzDYGpVToCKUlBqVUiAh6MIjIJNfp2YUisjgE2vO/InJaRA64dQu5U8xFJFlEPhGRfBHJE5FHQrGtIhIjIrtFZJ+rnU+FYjvdPjtCRL4UkY9DvJ2BvRQCyaAVgAgARQAGAYgGsA/AkCC36QYAwwEccOv2AoDFrueLATzvej7E1eYeAAa6piWim9qZCGC463k8gAJXe0KqrWg+dybO9TwKwOcAfhxq7XRr73wAbwP4OFT/9q7PPwrgsvO6+a2twV5iGAWgkOQ3JBsBvIvm07aDhuRWAN+d1znkTjEnWUbyP67n1QDy0XwWa0i1lc1qXC+jXMVQaycAiMiVAG4G8Jpb55Brpxd+a2uwg8HSKdohoEunmAeaiKQCGIbm/8Yh11bX4vlXaD7RbiPJkGwngP8BsAiA4dYtFNsJBOBSCO6snnYdKJZO0Q5hQW+/iMQBeB/AoyTPuc5p8dirh27d0lY2nyszVER6ofm8m6u99B6UdorIFACnSe4VkXFWBvHQrTv/9n6/FIK7YC8xhMsp2iF5irmIRKE5FN4i+bdQbisAkKwE8CmaL/kXau0cC2CqiBxF8ypthoi8GYLtBBD4SyEEOxi+AJAmIgNFJBrN14pcH+Q2eRJyp5hL86LBagD5JF8K1baKyOWuJQWIyCUAxgM4FGrtJPk4yStJpqL5e7iF5IxQayfQTZdC6K6tqF62rv4MzVvUiwA8EQLteQdAGYAmNCftbAB90HzB2yOuxwS3/p9wtf0wgMnd2M6foHlx8GsAX7nqZ6HWVgDXAvjS1c4DAJa6uodUO89r8zj8316JkGsnmvfi7XNVXsvvxp9t1SMflVImwV6VUEqFIA0GpZSJBoNSykSDQSllosGglDLRYFBKmWgwKKVMNBiUUib/H3f+NaGQgQzGAAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]}]}